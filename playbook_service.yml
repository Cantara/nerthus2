---
- name: Service configuration
  hosts: localhost
  connection: local
  vars:
    visuale: "https://visuale.cantara.no/api/status/"
    host: "cantara-lab-tokyo-wamper-1"
    env: "Tokyo_LAB"
    service: "wamper"
    zone: "lab.cantara.infra"
    service_type: "A2A"
    service_tag: "SoftwareFactory"
    buri_base_version: "0.3.5"
    webserver_port: 3030
    debug_port: 6060
  tasks:
    - set_fact:
        visuale: "{{ visuale }}/{{ env }}/{{ service }}/{{ host }}?service_tag={{ service_tag }}&service_type={{ service_type }}"
    - debug: var=hostvars[inventory_hostname]['ansible_default_ipv4']['address']
    - debug: var=hostvars[inventory_hostname]['ansible_default_ipv6']['address']
    - name: Installing Docker
      yum:
        name: docker
        state: latest
    - name: Starting and Enabling Docker service
      service:
        name: docker
        state: started
        enabled: yes
    - name: Add docker group to ec2-user
      ansible.builtin.user:
        name: ec2-user
        groups: docker
        append: yes
    - name: Create a eventstore container
      community.docker.docker_container:
        name: eventstore
        image: eventstore/eventstore
        pull: true
        volumes:
          - /var/lib/eventstore
          - /var/log/eventstore
        restart_policy: always
        ports:
          - 2113:2113
          - 1113:1113
          - 2112:2112
          - 1112:1112
        env:
          - EVENTSTORE_INSECURE: true
          - EVENTSTORE_EXT_IP: 0.0.0.0
          - EVENTSTORE_EXT_HOST_ADVERTISE_AS: "$(hostname -I | cut -f1 -d' ')"
          - EVENTSTORE_INT_IP: 0.0.0.0
          - EVENTSTORE_INT_HOST_ADVERTISE_AS: "$(hostname -I | cut -f1 -d' ')"
          - EVENTSTORE_CLUSTER_SIZE: 3
          - EVENTSTORE_CLUSTER_DNS: "{{ service }}.{{ zone }}"
    - name: Install chrome
      ansible.builtin.shell: |
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
        yum localinstall -y google-chrome-stable_current_x86_64.rpm
        rm google-chrome-stable_current_x86_64.rpm
    - name: Add service user {{ service }}
      ansible.builtin.user:
        name: "{{ service }}"
        comment: "User for {{ service }}"
        home: "/home/{{ service }}"
    - name: Check if buri exists
      stat:
        path: "/home/{{ service }}/buri"
      register: buri
    - name: Download buri
      ansible.builtin.get_url:
        url: "https://mvnrepo.cantara.no/content/repositories/releases/no/cantara/gotools/buri/v{{ buri_base_version }}/buri-v{{ buri_base_version }}"
        dest: "/home/{{ service }}/"
      register: buri_new
      when: keyfile.stat.exists == false
    - name: Create symbolic link for buri
      file:
        src: "{{ buri_new.dest }}"
        dest: "/home/{{ service }}/buri"
        state: link
      when: keyfile.stat.exists == false
    - name: Set report properties
      copy:
        content: |
          healthUrl='http://localhost:{{ port }}/{{ service }}/health'
          reportToUrl1='{{ visuale }}'
          #reportToUrl2='{{ visuale }}'
        dest: "/home/{{ service }}/scripts/reportServiceHealthToVisuale.properties"
    - name: Set service cron file
      copy:
        content: |
          MAILTO=""
          */6 * * * * ./buri -a buri -g no/cantara/gotools > /dev/null
          */6 * * * * ./buri -a "{{ service }}" -g no/cantara/gotools -r > /dev/null
          * * * * * ./scripts/reportServiceHealthToVisuale.sh > /dev/null
        dest: "/home/{{ service }}/CRON"
    - name: Remove cronjob from crontab scheduler
      shell: crontab -r
    - name: Configure cronjob via crontab scheduler
      shell: "crontab /home/{{ service }}/CRON"
    - name: Listing cronjobs via crontab scheduler
      shell: crontab -l
    - name: Set env file
      copy:
        content: |
          webserver.port={{ webserver_port }}
          debug.port={{ debug_server }}

          inmem=false

          debug.user=labDebug
          debug.pass=syuperDebugLab

          screenshot.key=MgqHE2qIgboCKxgavuH/n1i36ddzkVRHTnsoGlM+IBI=
          screenshot.service.key=NkyQQ5mjRTTZq6e3c2vkv+JVWmgJAvjddtffYLDJWXM=
          slack.service.key=10igBbnXQymZ5lhLopBGeJNO+RO7MzBlrtnY3TgnvTY=
          authkey=b3+6ErSirqLVSrRI7pBS/VHqaCza8TtQOl5g1Lg/PYM=
        dest: "/home/{{ service }}/.env"



    - name: Create VPC
      ec2_vpc_net:
        name: "{{ vpc_name }}"
        cidr_block: "{{ vpc_cidr }}"
        ipv6_cidr: true
        region: "{{ region }}"
        state: present
      register: vpc
    - name: Ansible | Print ipv6
      debug:
        msg: "{{ vpc.vpc.ipv6_cidr_block_association_set[0].ipv6_cidr_block[:17] }}{{ index }}::/64"
      loop: "{{ subnets | flatten(levels=1) }}"
      loop_control:
        index_var: index
    - name: Associate subnet to the VPC
      ec2_vpc_subnet:
        state: present
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ region }}"
        cidr: "{{ item.cidr }}"
        assign_instances_ipv6: true
        ipv6_cidr: "{{ vpc.vpc.ipv6_cidr_block_association_set[0].ipv6_cidr_block[:17] }}{{ index }}::/64"
        az: "{{ item.az }}"
        tags:
          Name: "{{ item.name }}"
      register: subnets
      loop: "{{ subnets | flatten(levels=1) }}"
      loop_control:
        index_var: index
    - name: Create ec2 vpc internet gateway
      ec2_vpc_igw:
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ region }}"
        state: present
      register: igw_result
    - name: Gather information about any VPC route table within VPC with ID vpc-abcdef00
      amazon.aws.ec2_vpc_route_table_info:
        filters:
          vpc-id: "{{ vpc.vpc.id }}"
      register: route_info
    - name: Ansible | Print route info
      debug:
        msg: "{{ route_info }}"
    - name: Create gateway route table
      amazon.aws.ec2_vpc_route_table:
        vpc_id: "{{ vpc.vpc.id }}"
        lookup: id
        route_table_id: "{{ route_info.route_tables[0].id }}"
        tags:
          Name: Gateway route table
        #gateway_id: "{{ igw_result.gateway_id }}"
        routes:
          - dest: 0.0.0.0/0
            gateway_id: "{{ igw_result.gateway_id }}"
          - dest: ::/0
            gateway_id: "{{ igw_result.gateway_id }}"
    - name: Create security group
      ec2_group:
        name: "{{ security_group_name }}"
        description: "Security group for instances"
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ region }}"
        rules:
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: 0.0.0.0/0
          - proto: all
            group_name: "{{ security_group_name }}"
        state: present
      register: security_group
    - name: Create a private zone
      amazon.aws.route53_zone:
        zone: "{{ zone }}"
        vpc_id: "{{ vpc.vpc.id }}"
        vpc_region: "{{ region }}"
        comment: DNS for wamper lab cantara
    - name: Gather Facts to check if a similar instance is running
      ec2_instance_info:
        filters:
          "tag:Name": "{{ item.name }}"
      register: ec2_exists
      with_items: "{{ instances }}"
    - name: Display Details
      debug: var=ec2_exists
    - set_fact:
        ip_addresses_old: "{{ ec2_exists.results | selectattr('instances', 'defined') | map(attribute='instances') | flatten | selectattr('state.name','equalto','running') | map(attribute='private_ip_address') | list }}"
    - name: Route 53 A Record Update Before Launch
      amazon.aws.route53:
        state: present
        zone: "{{ zone }}"
        private_zone: true
        record: "{{ service }}.{{ zone }}"
        type: A
        ttl: 20
        value: "{{ ip_addresses_old }}"
        wait: yes
        overwrite: yes
      when: ip_addresses_old | count >= 1
    - name: Route 53 A Record Deleting Before Launch
      amazon.aws.route53:
        state: absent
        zone: "{{ zone }}"
        private_zone: true
        record: "{{ service }}.{{ zone }}"
        type: A
        ttl: 20
        wait: yes
        overwrite: yes
      when: ip_addresses_old | count == 0
    - name: Launch instances
      ec2_instance:
        key_name: "{{ key_name }}"
        security_group: "{{ security_group_name }}"
        count: 1
        instance_type: t3.small #t4g.nano #t3.micro
        image_id: "{{ item.ami }}"
        region: "{{ region }}"
        vpc_subnet_id: "{{ subnets.results[index].subnet.id }}"
        network:
          assign_public_ip: true
        volumes:
          - device_name: /dev/xvda
            ebs:
              volume_size: 20
              volume_type: "gp3"
        tags:
          Name: "{{ item.name }}"
        state: started
        wait: true
        user_data: |
          #!/bin/env sh
          yum install -y ansible > /dev/null
          
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
          yum localinstall -y google-chrome-stable_current_x86_64.rpm
          rm google-chrome-stable_current_x86_64.rpm
          
          amazon-linux-extras enable ansible2
          yum install -y docker > /dev/null
          systemctl start docker
          systemctl enable docker
          #sleep 15s
          addr=$(hostname -I | cut -f1 -d' ')
          docker run -d --restart=always --name eventstore -p 2113:2113 -p 1113:1113 -p 2112:2112 -p 1112:1112 eventstore/eventstore:latest --insecure --cluster-size=3 --cluster-dns={{ service }}.{{ zone }} --int-ip=0.0.0.0 --int-host-advertise-as=$addr --ext-ip=0.0.0.0 --ext-host-advertise-as=$addr
          #docker run -d --restart=always --name eventstore -p 2113:2113 -p 1113:1113 eventstore/eventstore:latest --insecure --cluster-size=3 --cluster-dns=wamper.lab.cantara.infra --run-projections=All --node-priority={{ index+1 }} --node-priority={{ index+1 }} --advertise-host-to-client-as=$addr
          #docker run -d --restart=always --name eventstore -p 2113:2113 -p 1113:1113 ghcr.io/eventstore/eventstore:20.6.1-alpha.0.69-arm64v8 --insecure --cluster-size=3 --cluster-dns=wamper.lab.cantara.infra --gossip-allowed-difference-ms=6000000 --gossip-timeout-ms=250000 --leader-election-timeout-ms=1000000
          usermod -a -G docker ec2-user
          
          adduser wamper

          cd /home/ec2-user
          cat <<'EOF' > ./CRON
          MAILTO=""
          */30 * * * * sudo yum update -y > /dev/null
          0 3 * * 6 sudo reboot
          EOF
          chown ec2-user CRON
          chgrp ec2-user CRON

          sudo -u ec2-user crontab ./CRON
          
          cat <<'EOF' > su_to_wamper.sh
          #!/bin/env sh
          sudo su - wamper
          EOF

          chmod +x su_to_wamper.sh
          chown ec2-user su_to_wamper.sh
          chgrp ec2-user su_to_wamper.sh
          
          #exit
          # Switch to user
          #./su_to_wamper.sh
          
          cd /home/wamper
          
          cat <<'SCREOF' > ./createWamper.sh
          mkdir eventmap

          # Install semantic-versioning and visuale
          curl -s "https://raw.githubusercontent.com/Cantara/visuale/master/agent/scripts/download_and_setup_visuale_reporting.sh" | bash -s

          rm ~/scripts/*_template

          cat <<'EOF' > ~/scripts/reportServiceHealthToVisuale.properties
          healthUrl=http://localhost:13030/wamper/health
          reportToUrl1='https://visuale.cantara.no/api/status/Tokyo_LAB/Wamper/cantara-lab-tokyo-wamper-{{ index +1 }}?service_tag=SoftwareFactory&service_type=A2A'
          #reportToUrl2='https://visuale.cantara.no/api/status/Tokyo_LAB/Wamper/cantara-lab-tokyo-wamper-{{ index +1 }}?service_tag=SoftwareFactory&service_type=A2A'
          EOF

          cat <<'EOF' > ~/scripts/CRON
          MAILTO=""
          */6 * * * * ./buri -a buri -g no/cantara/gotools > /dev/null
          */6 * * * * ./buri -a wamper -g no/cantara/gotools -r > /dev/null
          * * * * * ./health-probe -r "https://visuale.quadim.ai/api/status/eXOR/Eventstore/exoreaction-lab-eventstore-{{ index +1 }}?service_tag=EventServices&service_type=A2A" -h "http://localhost:13030/wamper/health" -t "go" > /dev/null &
          #* * * * * ./scripts/reportServiceHealthToVisuale.sh > /dev/null
          EOF

          ln -s scripts/CRON CRON

          crontab ~/CRON

          curl --fail --show-error --silent -o "buri-v0.3.5" "https://mvnrepo.cantara.no/content/repositories/releases/no/cantara/gotools/buri/v0.3.5/buri-v0.3.5"
          ln -s "buri-v0.3.5" "buri"
          chmod +x "buri"

          cat <<'EOF' > ~/.env
          webserver.port=13030
          debug.port=6060
          
          eventstore.host=eventstore.{{ zone }}

          debug.user=labDebug
          debug.pass=syuperDebugLab

          screenshot.key=MgqHE2qIgboCKxgavuH/n1i36ddzkVRHTnsoGlM+IBI=
          screenshot.service.key=NkyQQ5mjRTTZq6e3c2vkv+JVWmgJAvjddtffYLDJWXM=
          slack.service.key=10igBbnXQymZ5lhLopBGeJNO+RO7MzBlrtnY3TgnvTY=
          authkey=b3+6ErSirqLVSrRI7pBS/VHqaCza8TtQOl5g1Lg/PYM=
          EOF

          ./buri -a buri -g no/cantara/gotools
          ./buri -a wamper -g no/cantara/gotools -r

          # Clear history which contains passwords and secrets
          echo '' > ~/.bash_history
          history -c
          exit
          
          SCREOF
          chown wamper createWamper.sh
          chgrp wamper createWamper.sh
          chmod +x createWamper.sh
          
          sudo -u ./createWamper.sh
      #register: _alias_vc_0
      register: instances
      async: 3600
      poll: 0
      with_items: "{{ instances }}"
      loop: "{{ instances | flatten(levels=1) }}"
      loop_control:
        index_var: index
      when:
        - ec2_exists.results[index].instances | count == 0 or ec2_exists.results[index].instances | selectattr('state.name','equalto','running') | list | count == 0
    - name: Wait for instances launch to finish
      async_status:
        jid: "{{ item.ansible_job_id }}"
      register: instances
      #register: instances
      retries: 500
      delay: 5
      until: instances.finished
      #until: instances.finished
      loop: "{{instances.results}}"
      when:
        - item.finished is defined
        - item.finished == 0
    - name: Display New Instances
      debug: var=instances
    - set_fact:
        ip_addresses_new: "{{ instances.results | selectattr('instances', 'defined') | map(attribute='instances') | flatten | map(attribute='private_ip_address') | list }}"
    - set_fact:
        ip_addresses: "{{ ip_addresses_new + ip_addresses_old }}"
    - name: Display IPs
      debug: var=ip_addresses
    - name: Route 53 A Record Update After Launch
      amazon.aws.route53:
        state: present
        zone: "{{ zone }}"
        private_zone: true
        record: "{{ service }}.{{ zone }}"
        type: A
        ttl: 20
        value: "{{ ip_addresses }}"
        wait: yes
        overwrite: yes
    - set_fact:
        instances_ids_old: "{{ ec2_exists.results | selectattr('instances', 'defined') | map(attribute='instances') | flatten | selectattr('state.name','equalto','running') | map(attribute='instance_id') | list }}"
        instances_ids_new: "{{ instances.results | selectattr('instances', 'defined') | map(attribute='instances') | flatten | selectattr('instance_id', 'defined') | map(attribute='instance_id') | list }}"
    - name: Display instance_ids_old
      debug:
        msg: "{{ instances_ids_old }}"
    - name: Display instance_ids_new
      debug:
        msg: "{{ instances_ids_new }}"
    - name: Create Application Load Balancer
      elb_application_lb:
        name: "{{ lb_name }}"
        state: present
        region: "{{ region }}"
        security_groups:
          - "{{ security_group.group_id }}"
        subnets: "{{ subnets.results | map(attribute='subnet.id') | list }}"
        listeners:
          - Protocol: HTTP
            Port: 80
            DefaultActions:
              - Type: redirect
                RedirectConfig:
                  Protocol: HTTPS
                  Port: "443"
                  Host: "#{host}"
                  Path: "/#{path}"
                  Query: "#{query}"
                  StatusCode: "HTTP_301"
      register: http_lb
    - name: Create Application Load Balancer Target Group
      elb_target_group:
        name: "{{ target_group_name }}"
        state: present
        region: "{{ region }}"
        protocol: HTTP
        port: 443
        vpc_id: "{{ vpc.vpc.id }}"
      register: target_group
    - name: Add instances to target group
      elb_target:
        state: present
        region: "{{ region }}"
        target_group_arn: "{{ target_group.target_group_arn }}"
        target_id: "{{ item }}"
      with_items: "{{ instances_ids_new + instances_ids_old }}"
